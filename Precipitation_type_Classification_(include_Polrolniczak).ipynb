{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Precipitation-type-Classification (include Polrolniczak).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "87m2tcgIpYGx",
        "JUC4zfMggWrQ",
        "qpQDNpAQh9gd",
        "SoJ71ikHiIiW",
        "yTvlpn01ijIk",
        "Yz9rtMdcjCw7",
        "mhe1wjcbfUUf",
        "e1_sZm6Vlx0q"
      ],
      "mount_file_id": "1J6sU3FKIb0oTylLOKZmEmMbz0fcfuxxK",
      "authorship_tag": "ABX9TyMUomn2k+d6SY8wi/qFqZfm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SojeongAn/tokyo/blob/main/Precipitation_type_Classification_(include_Polrolniczak).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI for precipitation detection (Baseline by Polrolniczak 2021).*\n",
        "\n",
        "This colab contains:\n",
        "* Code to read the dataset using [sklearn and Deep learning module (Pytorch, Tensorflow)]\n",
        "* Example code to load this model and use it to make predictions ([github](https://github.com/SojeongAn/tokyo)).\n",
        "\n",
        "It has been tested in a public Google colab kernel."
      ],
      "metadata": {
        "id": "OoZaOQ9Bfs25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Access your google drive\n"
      ],
      "metadata": {
        "id": "87m2tcgIpYGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KO35GOBcpYR2",
        "outputId": "f7b7a591-5581-45e0-d98d-da51b8e36d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library dependency installs and imports\n",
        "\n"
      ],
      "metadata": {
        "id": "JUC4zfMggWrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BASIC LIBRARIES \n",
        "import os\n",
        "import pickle\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import netCDF4 as nc"
      ],
      "metadata": {
        "id": "fLoRlyFMfslH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SKLEARN MODEL\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn import tree\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neural_network import MLPRegressor"
      ],
      "metadata": {
        "id": "P-Y898chd3g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Load and pre-processing"
      ],
      "metadata": {
        "id": "qpQDNpAQh9gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class snowData():\n",
        "\n",
        "    def __init__(\n",
        "        self\n",
        "    ):\n",
        "        self.diff = dt.timedelta(hours=9)\n",
        "        self.interval = dt.timedelta(hours=1)\n",
        "\n",
        "\n",
        "    def readData(self, fname):\n",
        "        with open(fname):\n",
        "            file = nc.Dataset(fname,mode='r')\n",
        "            for v in file.variables.keys():\n",
        "                var = file.variables[v]\n",
        "                print(var)\n",
        "\n",
        "\n",
        "    def loadingData(self, year, month):\n",
        "        if month == 12:\n",
        "            day_count = 31\n",
        "        else:\n",
        "            day_count = (dt.date(year, month+1, 1) - dt.date(year, month, 1)).days\n",
        "        \n",
        "        for i in range(0, day_count):\n",
        "            day = i+1\n",
        "\n",
        "            for j in range(0, 6):\n",
        "                start_date = dt.datetime(year, month, day, 0, 0)\n",
        "                kst = (start_date + j * self.interval) \n",
        "                utc = kst - self.diff\n",
        "\n",
        "                ym = utc.strftime(\"%Y%m\")\n",
        "                ymdhm = utc.strftime(\"%Y%m%d_%H%M\")\n",
        "                mlfile = 'data/{}/ERA5_anal_ml_{}.nc'.format(ym, ymdhm)\n",
        "                sfcfile = 'data/{}/ERA5_anal_sfc_{}.nc'.format(ym, ymdhm)\n",
        "\n",
        "                if not os.path.isfile(sfc):\n",
        "                    pass\n",
        "\n",
        "                ml = self.mlDataset(mlfile)\n",
        "                sfc = self.sfcDataset(sfcfile)\n",
        "\n",
        "                data = ml.update(sfc)\n",
        "                ymdh = kst.strftime(\"%Y%m%d%H\")\n",
        "                save_path = os.path.join('data/prepro', ymdh + '.pickle')\n",
        "                with open(save_path,'wb') as fw:\n",
        "                    pickle.dump(data, fw)\n",
        "\n",
        "\n",
        "    def mlDataset(self, fname):\n",
        "\n",
        "        with open(fname):\n",
        "            ds = nc.Dataset(fname, mode='r')\n",
        "            time = ds.variables['time']\n",
        "            time_ = nc.num2date(time[:], time.units, time.calendar)\n",
        "            time_ = dt.datetime.strptime(str(time_[0]),'%Y-%m-%d %H:%M:%S')\n",
        "            time_ = (time_+self.diff).strftime(\"%Y%m%d%H%M\")\n",
        "            crwc = ds.variables['crwc'][:].reshape(-1)\n",
        "            cswc = ds.variables['cswc'][:].reshape(-1)\n",
        "            etadot = ds.variables['etadot'][:].reshape(-1)\n",
        "            z = ds.variables['z'][:].reshape(-1)\n",
        "            t = ds.variables['t'][:].reshape(-1)\n",
        "            q = ds.variables['q'][:].reshape(-1)\n",
        "            w = ds.variables['w'][:].reshape(-1)\n",
        "            vo = ds.variables['vo'][:].reshape(-1)\n",
        "            lnsp = ds.variables['lnsp'][:].reshape(-1)\n",
        "            d = ds.variables['d'][:].reshape(-1)\n",
        "            u = ds.variables['u'][:].reshape(-1)\n",
        "            v = ds.variables['v'][:].reshape(-1)\n",
        "            o3 = ds.variables['o3'][:].reshape(-1)\n",
        "            clwc = ds.variables['clwc'][:].reshape(-1)\n",
        "            ciwc = ds.variables['ciwc'][:].reshape(-1)\n",
        "            cc = ds.variables['cc'][:].reshape(-1)\n",
        "        mldict = {\n",
        "                'time': time_, 'crwc': crwc, 'cswc': cswc, \n",
        "                'etadot': etadot, 'z': z, 't': t, 'q': q, 'w': w, \n",
        "                'vo': vo, 'lnsp': lnsp, 'd': d, 'u': u, 'v': v, \n",
        "                'o3': o3, 'clwc': clwc, 'ciwc': ciwc, 'cc': cc\n",
        "                }\n",
        "        return mldict\n",
        "\n",
        "\n",
        "    def sfcDataset(self, fname):\n",
        "        with open(fname):\n",
        "            ds = nc.Dataset(fname, mode='r')\n",
        "            lsm = ds.variables['lsm'][:]\n",
        "            siconc = ds.variables['siconc'][:]\n",
        "            asn = ds.variables['asn'][:]\n",
        "            rsn = ds.variables['rsn'][:]\n",
        "            sst = ds.variables['sst'][:]\n",
        "            sp = ds.variables['sp'][:]\n",
        "            sd = ds.variables['sd'][:]\n",
        "            msl = ds.variables['msl'][:]\n",
        "            blh = ds.variables['blh'][:]\n",
        "            tcc = ds.variables['tcc'][:]\n",
        "            u10 = ds.variables['u10'][:]\n",
        "            v10 = ds.variables['v10'][:]\n",
        "            t2m = ds.variables['t2m'][:]\n",
        "            d2m = ds.variables['d2m'][:]\n",
        "            lcc = ds.variables['lcc'][:]\n",
        "            mcc = ds.variables['mcc'][:]\n",
        "            hcc = ds.variables['hcc'][:]\n",
        "            skt = ds.variables['skt'][:]\n",
        "            swvl1 = ds.variables['swvl1'][:]\n",
        "            swvl2 = ds.variables['swvl2'][:]\n",
        "            swvl3 = ds.variables['swvl3'][:]\n",
        "            swvl4 = ds.variables['swvl4'][:]\n",
        "            stl1 = ds.variables['stl1'][:]\n",
        "            stl2 = ds.variables['stl2'][:]\n",
        "            stl3 = ds.variables['stl3'][:]\n",
        "            stl4 = ds.variables['stl4'][:]\n",
        "        sfcdict = {\n",
        "                'lsm': lsm, 'siconc': siconc, 'asn': asn, \n",
        "                'rsn': rsn, 'sst': sst, 'sp': sp, 'sd': sd, \n",
        "                'msl': msl, 'blh': blh, 'tcc': tcc, 'u10': u10, \n",
        "                'v10': v10, 't2m': t2m, 'd2m': d2m, 'lcc': lcc, \n",
        "                'mcc': mcc, 'hcc': hcc, 'skt': skt, 'swvl1': swvl1, \n",
        "                'swvl2': swvl2, 'swvl3': swvl3,'swvl4': swvl4, \n",
        "                'stl1': stl1, 'stl2': stl2, 'stl3': stl3, 'stl4': stl4\n",
        "                }\n",
        "        return sfcdict\n",
        "\n",
        "            \n",
        "if __name__ == \"__main__\":\n",
        "    y, m = 2018, 1\n",
        "    sd = snowData()\n",
        "    sd.loadingData(y, m)\n"
      ],
      "metadata": {
        "id": "tnXdrTUBfRtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if __name__ == \"__main__\":\n",
        "mdict = [1, 2, 12]\n",
        "sd = snowData()\n",
        "for i in range(3):\n",
        "    for m in mdict:\n",
        "        yy, mm = 2018+i, m\n",
        "        sd.loadingData(yy, mm)"
      ],
      "metadata": {
        "id": "1yIyZrgxwgfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading from CSV-file processed"
      ],
      "metadata": {
        "id": "19wySsVKiC4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"PROCESSED_DATA\", header=None)"
      ],
      "metadata": {
        "id": "6wmMDBpAiISJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> List of features\n",
        "1.  Temperature related parameters : LR03, T2, T10, T100, T250 , T500 , T1000 , T1500 , T2000 , T2500 , T3000 , ISO_0_HGT , WLD01\n",
        "2.  Humidity parameters : Q2, Q10, Q100, Q250, Q500, Q1000, Q1500, Q2000, Q2500, Q3000,\n",
        "3.  Wind parameters : WS10, WS100, WS250, WS500, WS1000, WS1500, WS2000, WS2500, WS3000, \n",
        "4.  Remote sensing data parameters : CMAX\n"
      ],
      "metadata": {
        "id": "T6gQHva6cNXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predict_data = [data[''][137, 133, ...], ....]\n"
      ],
      "metadata": {
        "id": "R0Li75VYWalV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Altitute according to model level ([REF](https://confluence.ecmwf.int/display/UDOC/L137+model+level+definitions))\n",
        "* 2m: \n",
        "* 10m: 137 (  10.00m)\n",
        "* 100m: 133 ( 106.54m)\n",
        "* 250m: 129 ( 244.68m)\n",
        "* 500m: 124 ( 500.01m)\n",
        "* 1000m: 116 ( 987.00m)\n",
        "* 1500m: 114 (1459.58m)\n",
        "* 2000m: 110 (2080.41m)\n",
        "* 3000m: 105 (3087.75m)"
      ],
      "metadata": {
        "id": "FOemo0E-WSJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "C-fVF6Z0cH9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training for snow-detection"
      ],
      "metadata": {
        "id": "SoJ71ikHiIiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DECISION TREE\n",
        "\n"
      ],
      "metadata": {
        "id": "yTvlpn01ijIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(x, y)"
      ],
      "metadata": {
        "id": "Mxp368eneRpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree.plot_tree(clf)"
      ],
      "metadata": {
        "id": "Xs4ByXsmex_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### REGRESSION\n",
        "\n"
      ],
      "metadata": {
        "id": "Yz9rtMdcjCw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg = LinearRegression().fit(x, y)"
      ],
      "metadata": {
        "id": "DeiYSBXujG3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AUTOENCODER\n",
        "\n"
      ],
      "metadata": {
        "id": "mhe1wjcbfUUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aut = MLPRegressor(hidden_layer_sizes = (n_encoder1, n_encoder2, n_latent, n_decoder2, n_decoder1), \n",
        "                   activation = 'tanh', \n",
        "                   solver = 'adam', \n",
        "                   learning_rate_init = 0.0001, \n",
        "                   max_iter = 20, \n",
        "                   tol = 0.0000001, \n",
        "                   verbose = True)\n",
        "aut.fit(x, y)"
      ],
      "metadata": {
        "id": "eia0zDqpfemp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "The most fundamental measures included the following: \n",
        " * Hit Rate (HR)\n",
        " * Proportion Corrrect (PC)\n",
        " * False Alarm Ratio (FAR)\n",
        " * Critical Succcess Index (CSI) \n",
        " * Equitable Threat Score (ETS)"
      ],
      "metadata": {
        "id": "e1_sZm6Vlx0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{align}\n",
        "CSI = \\frac{hits}{hits+misses+falsealarms}  \n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "xJbpgvkdkLJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(vecY):\n",
        "    for i in range(0, len(vecY)):\n",
        "        if(int(vecY[i]) == 0):\n",
        "            vecY[i] = 1 # label == 0, return 1\n",
        "        else:\n",
        "            vecY[i] = -1 # label != 0, return -1\n",
        "    return vecY"
      ],
      "metadata": {
        "id": "icJ20-U9lwwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{align}\n",
        "ETS &= \\frac{hits-hits_{ranodm}}{hits+misses+false alarms-hits_{random}}  \\\\[1em]\n",
        "hits_{random} &= \\frac{(hits+misses)(guts+false alarm)}{total}  \n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "f0rAW-Y_g04E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jUSepVRtkIjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization\n",
        "\n",
        "Frequently used visualization methods included the following: \n",
        " * "
      ],
      "metadata": {
        "id": "fP59RusukiD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ucm_W0jSlp_k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}